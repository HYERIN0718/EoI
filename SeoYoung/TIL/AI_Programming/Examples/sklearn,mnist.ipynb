{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOs2pJS18FSqHlG7AFavt2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WESTZERO115/EoI/blob/main/SeoYoung/TIL/AI_Programming/Examples/sklearn%2Cmnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SJ8D4_4mN-n",
        "outputId": "a5bdbe33-70e8-4127-d19c-3218177d8125"
      },
      "source": [
        "## sklearn의 필기 숫자 데이터셋\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "digit = datasets.load_digits()\n",
        "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size=0.6)\n",
        "\n",
        "#MLP분류기 모델을 학습\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100), learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd', verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "res = mlp.predict(x_test)\n",
        "\n",
        "conf = np.zeros((10,10))\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]] +=1\n",
        "print(conf)\n",
        "\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "  no_correct +=conf[i][i]\n",
        "accuracy = no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.ㅁ\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.09663146\n",
            "Iteration 2, loss = 0.32687623\n",
            "Iteration 3, loss = 0.21006348\n",
            "Iteration 4, loss = 0.15603893\n",
            "Iteration 5, loss = 0.11343944\n",
            "Iteration 6, loss = 0.09568080\n",
            "Iteration 7, loss = 0.08253247\n",
            "Iteration 8, loss = 0.06885157\n",
            "Iteration 9, loss = 0.05862781\n",
            "Iteration 10, loss = 0.05212023\n",
            "Iteration 11, loss = 0.04599399\n",
            "Iteration 12, loss = 0.04238666\n",
            "Iteration 13, loss = 0.03918988\n",
            "Iteration 14, loss = 0.03570029\n",
            "Iteration 15, loss = 0.03360512\n",
            "Iteration 16, loss = 0.03073997\n",
            "Iteration 17, loss = 0.02896052\n",
            "Iteration 18, loss = 0.02744930\n",
            "Iteration 19, loss = 0.02611325\n",
            "Iteration 20, loss = 0.02412215\n",
            "Iteration 21, loss = 0.02285858\n",
            "Iteration 22, loss = 0.02210313\n",
            "Iteration 23, loss = 0.02092713\n",
            "Iteration 24, loss = 0.02010945\n",
            "Iteration 25, loss = 0.01898622\n",
            "Iteration 26, loss = 0.01818232\n",
            "Iteration 27, loss = 0.01736644\n",
            "Iteration 28, loss = 0.01678762\n",
            "Iteration 29, loss = 0.01617443\n",
            "Iteration 30, loss = 0.01553210\n",
            "Iteration 31, loss = 0.01491839\n",
            "Iteration 32, loss = 0.01448406\n",
            "Iteration 33, loss = 0.01400655\n",
            "Iteration 34, loss = 0.01341256\n",
            "Iteration 35, loss = 0.01327428\n",
            "Iteration 36, loss = 0.01268225\n",
            "Iteration 37, loss = 0.01234017\n",
            "Iteration 38, loss = 0.01211257\n",
            "Iteration 39, loss = 0.01161278\n",
            "Iteration 40, loss = 0.01152483\n",
            "Iteration 41, loss = 0.01108669\n",
            "Iteration 42, loss = 0.01074523\n",
            "Iteration 43, loss = 0.01048327\n",
            "Iteration 44, loss = 0.01020284\n",
            "Iteration 45, loss = 0.01000781\n",
            "Iteration 46, loss = 0.00967785\n",
            "Iteration 47, loss = 0.00956898\n",
            "Iteration 48, loss = 0.00937796\n",
            "Iteration 49, loss = 0.00918893\n",
            "Iteration 50, loss = 0.00890865\n",
            "Iteration 51, loss = 0.00872506\n",
            "Iteration 52, loss = 0.00874700\n",
            "Iteration 53, loss = 0.00838340\n",
            "Iteration 54, loss = 0.00821362\n",
            "Iteration 55, loss = 0.00807366\n",
            "Iteration 56, loss = 0.00792703\n",
            "Iteration 57, loss = 0.00784873\n",
            "Iteration 58, loss = 0.00772612\n",
            "Iteration 59, loss = 0.00749780\n",
            "Iteration 60, loss = 0.00739818\n",
            "Iteration 61, loss = 0.00720850\n",
            "Iteration 62, loss = 0.00709113\n",
            "Iteration 63, loss = 0.00695106\n",
            "Iteration 64, loss = 0.00688435\n",
            "Iteration 65, loss = 0.00674724\n",
            "Iteration 66, loss = 0.00661836\n",
            "Iteration 67, loss = 0.00649698\n",
            "Iteration 68, loss = 0.00640531\n",
            "Iteration 69, loss = 0.00632961\n",
            "Iteration 70, loss = 0.00621986\n",
            "Iteration 71, loss = 0.00614586\n",
            "Iteration 72, loss = 0.00606514\n",
            "Iteration 73, loss = 0.00604839\n",
            "Iteration 74, loss = 0.00594193\n",
            "Iteration 75, loss = 0.00579028\n",
            "Iteration 76, loss = 0.00571547\n",
            "Iteration 77, loss = 0.00563198\n",
            "Iteration 78, loss = 0.00556173\n",
            "Iteration 79, loss = 0.00548006\n",
            "Iteration 80, loss = 0.00544325\n",
            "Iteration 81, loss = 0.00537993\n",
            "Iteration 82, loss = 0.00527374\n",
            "Iteration 83, loss = 0.00527594\n",
            "Iteration 84, loss = 0.00516172\n",
            "Iteration 85, loss = 0.00506961\n",
            "Iteration 86, loss = 0.00502832\n",
            "Iteration 87, loss = 0.00495729\n",
            "Iteration 88, loss = 0.00488266\n",
            "Iteration 89, loss = 0.00485174\n",
            "Iteration 90, loss = 0.00482385\n",
            "Iteration 91, loss = 0.00473120\n",
            "Iteration 92, loss = 0.00468743\n",
            "Iteration 93, loss = 0.00462460\n",
            "Iteration 94, loss = 0.00456968\n",
            "Iteration 95, loss = 0.00451560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[70.  0.  0.  0.  0.  1.  1.  0.  0.  0.]\n",
            " [ 0. 74.  0.  0.  0.  0.  0.  0.  1.  1.]\n",
            " [ 0.  0. 67.  0.  0.  1.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0. 82.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. 65.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  3.  0. 66.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0. 73.  0.  0.  0.]\n",
            " [ 0.  0.  1.  0.  0.  0.  0. 65.  1.  0.]\n",
            " [ 0.  1.  0.  0.  0.  0.  1.  0. 69.  1.]\n",
            " [ 0.  0.  0.  0.  2.  2.  0.  0.  0. 69.]]\n",
            "테스트 집합에 대한 정확률은  97.35744089012516 %입니다.ㅁ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2caIgBSn6w8",
        "outputId": "d21e6c7c-da34-4ed6-8db8-1571a20846d0"
      },
      "source": [
        "## MNIST 데이터셋으로 확장하기\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "mnist.data = mnist.data/255.0\n",
        "x_train = mnist.data[:60000]; x_test = mnist.data[60000:]\n",
        "y_train = np.int16(mnist.target[:60000]); y_test = np.int16(mnist.target[60000:])\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose = True)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "res = mlp.predict(x_test)\n",
        "\n",
        "conf = np.zeros((10,10), dtype=np.int16)\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]]+=1\n",
        "print(conf)\n",
        "\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "  no_correct += conf[i][i]\n",
        "accuracy = no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.63849624\n",
            "Iteration 2, loss = 0.26928542\n",
            "Iteration 3, loss = 0.21294676\n",
            "Iteration 4, loss = 0.17831042\n",
            "Iteration 5, loss = 0.15250227\n",
            "Iteration 6, loss = 0.13298147\n",
            "Iteration 7, loss = 0.11768703\n",
            "Iteration 8, loss = 0.10443399\n",
            "Iteration 9, loss = 0.09335912\n",
            "Iteration 10, loss = 0.08415303\n",
            "Iteration 11, loss = 0.07635556\n",
            "Iteration 12, loss = 0.07031230\n",
            "Iteration 13, loss = 0.06386557\n",
            "Iteration 14, loss = 0.05886228\n",
            "Iteration 15, loss = 0.05458928\n",
            "Iteration 16, loss = 0.05005899\n",
            "Iteration 17, loss = 0.04631707\n",
            "Iteration 18, loss = 0.04282557\n",
            "Iteration 19, loss = 0.03978212\n",
            "Iteration 20, loss = 0.03728687\n",
            "Iteration 21, loss = 0.03537467\n",
            "Iteration 22, loss = 0.03272523\n",
            "Iteration 23, loss = 0.03045585\n",
            "Iteration 24, loss = 0.02865137\n",
            "Iteration 25, loss = 0.02605228\n",
            "Iteration 26, loss = 0.02418745\n",
            "Iteration 27, loss = 0.02230984\n",
            "Iteration 28, loss = 0.02105596\n",
            "Iteration 29, loss = 0.01944794\n",
            "Iteration 30, loss = 0.01825936\n",
            "Iteration 31, loss = 0.01725444\n",
            "Iteration 32, loss = 0.01593723\n",
            "Iteration 33, loss = 0.01517499\n",
            "Iteration 34, loss = 0.01397058\n",
            "Iteration 35, loss = 0.01316944\n",
            "Iteration 36, loss = 0.01189376\n",
            "Iteration 37, loss = 0.01123527\n",
            "Iteration 38, loss = 0.01034267\n",
            "Iteration 39, loss = 0.00951368\n",
            "Iteration 40, loss = 0.00895325\n",
            "Iteration 41, loss = 0.00843920\n",
            "Iteration 42, loss = 0.00777766\n",
            "Iteration 43, loss = 0.00725348\n",
            "Iteration 44, loss = 0.00677564\n",
            "Iteration 45, loss = 0.00624597\n",
            "Iteration 46, loss = 0.00618304\n",
            "Iteration 47, loss = 0.00564224\n",
            "Iteration 48, loss = 0.00521096\n",
            "Iteration 49, loss = 0.00475141\n",
            "Iteration 50, loss = 0.00448232\n",
            "Iteration 51, loss = 0.00411883\n",
            "Iteration 52, loss = 0.00400588\n",
            "Iteration 53, loss = 0.00406833\n",
            "Iteration 54, loss = 0.00347208\n",
            "Iteration 55, loss = 0.00347206\n",
            "Iteration 56, loss = 0.00294764\n",
            "Iteration 57, loss = 0.00276483\n",
            "Iteration 58, loss = 0.00261033\n",
            "Iteration 59, loss = 0.00245168\n",
            "Iteration 60, loss = 0.00227063\n",
            "Iteration 61, loss = 0.00218762\n",
            "Iteration 62, loss = 0.00199601\n",
            "Iteration 63, loss = 0.00188925\n",
            "Iteration 64, loss = 0.00184007\n",
            "Iteration 65, loss = 0.00168625\n",
            "Iteration 66, loss = 0.00163777\n",
            "Iteration 67, loss = 0.00162257\n",
            "Iteration 68, loss = 0.00151407\n",
            "Iteration 69, loss = 0.00135356\n",
            "Iteration 70, loss = 0.00129806\n",
            "Iteration 71, loss = 0.00127177\n",
            "Iteration 72, loss = 0.00120347\n",
            "Iteration 73, loss = 0.00114626\n",
            "Iteration 74, loss = 0.00109547\n",
            "Iteration 75, loss = 0.00101301\n",
            "Iteration 76, loss = 0.00604208\n",
            "Iteration 77, loss = 0.00555501\n",
            "Iteration 78, loss = 0.00211840\n",
            "Iteration 79, loss = 0.00102026\n",
            "Iteration 80, loss = 0.00088908\n",
            "Iteration 81, loss = 0.00084243\n",
            "Iteration 82, loss = 0.00079744\n",
            "Iteration 83, loss = 0.00076694\n",
            "Iteration 84, loss = 0.00074522\n",
            "Iteration 85, loss = 0.00072375\n",
            "Iteration 86, loss = 0.00070153\n",
            "Iteration 87, loss = 0.00067888\n",
            "Iteration 88, loss = 0.00066537\n",
            "Iteration 89, loss = 0.00065244\n",
            "Iteration 90, loss = 0.00063596\n",
            "Iteration 91, loss = 0.00061559\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[ 970    0    5    0    2    2    5    2    3    2]\n",
            " [   0 1125    2    0    0    1    2    3    0    2]\n",
            " [   1    4 1004    3    4    0    0    7    2    0]\n",
            " [   1    0    3  987    1   10    1    2    7    4]\n",
            " [   1    0    3    0  954    0    6    1    2    6]\n",
            " [   0    1    0    5    1  864    8    0    3    4]\n",
            " [   1    2    2    0    4    7  934    0    1    0]\n",
            " [   1    1    8    4    4    2    0 1007    4    1]\n",
            " [   4    2    4    5    0    3    2    3  950    3]\n",
            " [   1    0    1    6   12    3    0    3    2  987]]\n",
            "테스트 집합에 대한 정확률은  97.82 %입니다.\n"
          ]
        }
      ]
    }
  ]
}